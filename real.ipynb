{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/bin/python\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import openai\n",
    "import concurrent.futures\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import wandb\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set the device to CPU since CUDA isn't available\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (1.59.7)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: wandb in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (0.19.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (2.32.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/envs/dev/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "# print(sys.executable)\n",
    "!pip install openai torch torchvision tqdm wandb scikit-learn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ openai successfully imported\n",
      "✓ torch successfully imported (version 2.5.1)\n",
      "  CUDA available: False\n",
      "✓ torchvision successfully imported (version 0.20.1)\n",
      "✓ tqdm successfully imported\n",
      "✓ wandb successfully imported\n",
      "✓ scikit-learn successfully imported (version 1.4.2)\n",
      "✓ requests successfully imported\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def check_imports():\n",
    "    try:\n",
    "        import openai\n",
    "        print(\"✓ openai successfully imported\")\n",
    "    except ImportError:\n",
    "        print(\"✗ openai not installed properly\")\n",
    "        \n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"✓ torch successfully imported (version {torch.__version__})\")\n",
    "        print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "    except ImportError:\n",
    "        print(\"✗ torch not installed properly\")\n",
    "        \n",
    "    try:\n",
    "        import torchvision\n",
    "        print(f\"✓ torchvision successfully imported (version {torchvision.__version__})\")\n",
    "    except ImportError:\n",
    "        print(\"✗ torchvision not installed properly\")\n",
    "        \n",
    "    try:\n",
    "        import tqdm\n",
    "        print(\"✓ tqdm successfully imported\")\n",
    "    except ImportError:\n",
    "        print(\"✗ tqdm not installed properly\")\n",
    "    try:\n",
    "        import wandb\n",
    "        print(\"✓ wandb successfully imported\")\n",
    "    except ImportError:\n",
    "        print(\"✗ wandb not installed properly\")\n",
    "        \n",
    "    try:\n",
    "        import sklearn\n",
    "        print(f\"✓ scikit-learn successfully imported (version {sklearn.__version__})\")\n",
    "    except ImportError:\n",
    "        print(\"✗ scikit-learn not installed properly\")\n",
    "        \n",
    "    try:\n",
    "        import requests\n",
    "        print(\"✓ requests successfully imported\")\n",
    "    except ImportError:\n",
    "        print(\"✗ requests not installed properly\")\n",
    "\n",
    "# Run the check\n",
    "check_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_unsplash_photos(client_id, save_dir, num_photos):\n",
    "    \"\"\"\n",
    "    Downloads random stock photos from Unsplash API.\n",
    "    \n",
    "    Args:\n",
    "        client_id (str): Your Unsplash API client ID\n",
    "        save_dir (str): Directory where photos will be saved\n",
    "        num_photos (int): Total number of photos to download\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to downloaded photos\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Setup API parameters\n",
    "    url = \"https://api.unsplash.com/photos/random\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Client-ID {client_id}\",\n",
    "        \"Accept-Version\": \"v1\"\n",
    "    }\n",
    "    \n",
    "    downloaded_paths = []\n",
    "    photos_to_download = num_photos\n",
    "\n",
    "    # Download photos in batches since API limits to 30 per request\n",
    "    while photos_to_download > 0:\n",
    "        try:\n",
    "            # Calculate batch size for this iteration\n",
    "            batch_size = min(30, photos_to_download)\n",
    "            \n",
    "            params = {\n",
    "                \"count\": batch_size,\n",
    "                \"query\": \"stock photo\",\n",
    "                \"orientation\": \"landscape\"  # Consistent image orientation\n",
    "            }\n",
    "            # Make API request with error handling\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()  # Raise exception for bad status codes\n",
    "            photos = response.json()\n",
    "            \n",
    "            # Download each photo in the batch\n",
    "            for photo in tqdm(photos, desc=\"Downloading photos\"):\n",
    "                try:\n",
    "                    img_url = photo['urls']['regular']\n",
    "                    img_response = requests.get(img_url)\n",
    "                    img_response.raise_for_status()\n",
    "                    \n",
    "                    img_name = os.path.join(save_dir, f\"{photo['id']}.jpg\")\n",
    "                    with open(img_name, 'wb') as f:\n",
    "                        f.write(img_response.content)\n",
    "                    \n",
    "                    downloaded_paths.append(img_name)\n",
    "\n",
    "                except (KeyError, requests.RequestException) as e:\n",
    "                    print(f\"Error downloading photo: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            photos_to_download -= len(photos)\n",
    "            \n",
    "            # Respect Unsplash API rate limits\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error making API request: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Successfully downloaded {len(downloaded_paths)} photos\")\n",
    "    return downloaded_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unsplash_download():\n",
    "    client_id = \"XTmL-fe-ko4CMi-wI-Huj_Z0vadoeby5dH9IKv_EAi8\"  # Replace with your actual client ID\n",
    "    save_dir = \"unsplash_photos\"\n",
    "    num_photos = 10\n",
    "    \n",
    "    try:\n",
    "        photo_paths = download_unsplash_photos(client_id, save_dir, num_photos)\n",
    "        print(f\"Download completed. Photos saved in {save_dir}\")\n",
    "        \n",
    "        # Verify the downloads\n",
    "        actual_files = os.listdir(save_dir)\n",
    "        print(f\"Number of files downloaded: {len(actual_files)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download process: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify your StockPhotoDetector class to explicitly handle CPU tensors\n",
    "class StockPhotoDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockPhotoDetector, self).__init__()\n",
    "        \n",
    "        # Reducing model complexity slightly since we're on CPU\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),  # Reduced from 32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # Reduced from 64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Reduced from 128\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 28 * 28, 256),  # Reduced from 512\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading photos: 100%|██████████| 30/30 [00:08<00:00,  3.56it/s]\n",
      "Downloading photos: 100%|██████████| 30/30 [00:05<00:00,  5.43it/s]\n",
      "Downloading photos: 100%|██████████| 30/30 [00:06<00:00,  4.54it/s]\n",
      "Downloading photos: 100%|██████████| 10/10 [00:01<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 100 photos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['unsplash_photos/R-ntN6PE-4w.jpg',\n",
       " 'unsplash_photos/Z7d3gwPMSRQ.jpg',\n",
       " 'unsplash_photos/BauovfUhLhY.jpg',\n",
       " 'unsplash_photos/WfrlYHgmLiM.jpg',\n",
       " 'unsplash_photos/PVkX5S08u4Y.jpg',\n",
       " 'unsplash_photos/lYMqE22gpXc.jpg',\n",
       " 'unsplash_photos/bmO2bPPcomg.jpg',\n",
       " 'unsplash_photos/HtqkHc8uhRI.jpg',\n",
       " 'unsplash_photos/_-Ofoh09q_o.jpg',\n",
       " 'unsplash_photos/7VSJvwH7dyg.jpg',\n",
       " 'unsplash_photos/zrMpx6q2fZ0.jpg',\n",
       " 'unsplash_photos/Vn6p6CVM2Q0.jpg',\n",
       " 'unsplash_photos/gVa7zpwsxZo.jpg',\n",
       " 'unsplash_photos/GEsUykZ1moU.jpg',\n",
       " 'unsplash_photos/KlCs1THkzdc.jpg',\n",
       " 'unsplash_photos/2IqKGn0pee4.jpg',\n",
       " 'unsplash_photos/jjAAiZFooQg.jpg',\n",
       " 'unsplash_photos/mrPm2HdS3lg.jpg',\n",
       " 'unsplash_photos/x9KFWUlMnSU.jpg',\n",
       " 'unsplash_photos/-W7PoC64tNI.jpg',\n",
       " 'unsplash_photos/-itqD54b02U.jpg',\n",
       " 'unsplash_photos/MYOyeR6WTxo.jpg',\n",
       " 'unsplash_photos/LQhcr0apQ_c.jpg',\n",
       " 'unsplash_photos/R-GPpK7Th2M.jpg',\n",
       " 'unsplash_photos/iqrBzwJEsRo.jpg',\n",
       " 'unsplash_photos/t3nwaZWoOvM.jpg',\n",
       " 'unsplash_photos/i9TGriScRD8.jpg',\n",
       " 'unsplash_photos/ppwxwTuCf7I.jpg',\n",
       " 'unsplash_photos/505eectW54k.jpg',\n",
       " 'unsplash_photos/xr4DZ6QyaTg.jpg',\n",
       " 'unsplash_photos/MNd-Rka1o0Q.jpg',\n",
       " 'unsplash_photos/-NHOw1_02Zg.jpg',\n",
       " 'unsplash_photos/ShPePxTpw4M.jpg',\n",
       " 'unsplash_photos/cStSHUpdApc.jpg',\n",
       " 'unsplash_photos/wr46TAs06TU.jpg',\n",
       " 'unsplash_photos/z4__LvEwl0o.jpg',\n",
       " 'unsplash_photos/93DS-e6h8t4.jpg',\n",
       " 'unsplash_photos/ukzHlkoz1IE.jpg',\n",
       " 'unsplash_photos/NLE9RCsxX3c.jpg',\n",
       " 'unsplash_photos/XGJBSkoqX_I.jpg',\n",
       " 'unsplash_photos/hn7AIcJhbfE.jpg',\n",
       " 'unsplash_photos/AMtddh4qJNE.jpg',\n",
       " 'unsplash_photos/8jNSRm1wsqs.jpg',\n",
       " 'unsplash_photos/uhfCamFYkzY.jpg',\n",
       " 'unsplash_photos/kxyw5hzq_YE.jpg',\n",
       " 'unsplash_photos/i9TGriScRD8.jpg',\n",
       " 'unsplash_photos/LhRf0SKPlIE.jpg',\n",
       " 'unsplash_photos/MKv9gQHpICM.jpg',\n",
       " 'unsplash_photos/xr4DZ6QyaTg.jpg',\n",
       " 'unsplash_photos/7B9lIF5TELg.jpg',\n",
       " 'unsplash_photos/WMrd7-CjyF0.jpg',\n",
       " 'unsplash_photos/BgAxk2QWTOA.jpg',\n",
       " 'unsplash_photos/dSaEVV6Qlmg.jpg',\n",
       " 'unsplash_photos/MYOyeR6WTxo.jpg',\n",
       " 'unsplash_photos/Op1cQ8u-tvE.jpg',\n",
       " 'unsplash_photos/tMA5H4w3WN8.jpg',\n",
       " 'unsplash_photos/e9geJhWBC5I.jpg',\n",
       " 'unsplash_photos/rYDiAOGTJkc.jpg',\n",
       " 'unsplash_photos/VhrOLXlqShc.jpg',\n",
       " 'unsplash_photos/dY7m7gihd6w.jpg',\n",
       " 'unsplash_photos/t59JGwg--GM.jpg',\n",
       " 'unsplash_photos/yk0cK1utILc.jpg',\n",
       " 'unsplash_photos/MKv9gQHpICM.jpg',\n",
       " 'unsplash_photos/oqStl2L5oxI.jpg',\n",
       " 'unsplash_photos/ShPePxTpw4M.jpg',\n",
       " 'unsplash_photos/taUfUHLylIo.jpg',\n",
       " 'unsplash_photos/xMtuEnWRa1w.jpg',\n",
       " 'unsplash_photos/BauovfUhLhY.jpg',\n",
       " 'unsplash_photos/2ta8OjluZuI.jpg',\n",
       " 'unsplash_photos/yGzT-pmLHpI.jpg',\n",
       " 'unsplash_photos/5E2DJszAJi0.jpg',\n",
       " 'unsplash_photos/Op1cQ8u-tvE.jpg',\n",
       " 'unsplash_photos/-NHOw1_02Zg.jpg',\n",
       " 'unsplash_photos/XGJBSkoqX_I.jpg',\n",
       " 'unsplash_photos/iqrBzwJEsRo.jpg',\n",
       " 'unsplash_photos/z4__LvEwl0o.jpg',\n",
       " 'unsplash_photos/Gf_KqXHU-PY.jpg',\n",
       " 'unsplash_photos/-itqD54b02U.jpg',\n",
       " 'unsplash_photos/k86Q90P9q9o.jpg',\n",
       " 'unsplash_photos/c7t4IBGExWU.jpg',\n",
       " 'unsplash_photos/QA7KTyc3G5Y.jpg',\n",
       " 'unsplash_photos/N6b1te27ykc.jpg',\n",
       " 'unsplash_photos/TamMbr4okv4.jpg',\n",
       " 'unsplash_photos/wr46TAs06TU.jpg',\n",
       " 'unsplash_photos/iIdpTCuu0OM.jpg',\n",
       " 'unsplash_photos/73ll0VcuayM.jpg',\n",
       " 'unsplash_photos/OWNYVarbnIc.jpg',\n",
       " 'unsplash_photos/s0CTzoewdEs.jpg',\n",
       " 'unsplash_photos/Vn6p6CVM2Q0.jpg',\n",
       " 'unsplash_photos/7TNFGpQ-0HQ.jpg',\n",
       " 'unsplash_photos/hZrmjktiv1c.jpg',\n",
       " 'unsplash_photos/HFhN0ynv4G4.jpg',\n",
       " 'unsplash_photos/Xu4eHONyOj4.jpg',\n",
       " 'unsplash_photos/xr4DZ6QyaTg.jpg',\n",
       " 'unsplash_photos/5E2DJszAJi0.jpg',\n",
       " 'unsplash_photos/6blHMaP5j6w.jpg',\n",
       " 'unsplash_photos/2H2_gDln5BI.jpg',\n",
       " 'unsplash_photos/l9Z93oauxgs.jpg',\n",
       " 'unsplash_photos/Z2M2KkdNLWQ.jpg',\n",
       " 'unsplash_photos/VhrOLXlqShc.jpg']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_unsplash_photos('XTmL-fe-ko4CMi-wI-Huj_Z0vadoeby5dH9IKv_EAi8', 'unsplash_photos', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'OPEN_API_KEY'\n",
    "\n",
    "def generate_dalle_images(num_images, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for i in range(num_images):\n",
    "        response = openai.Image.create(prompt=\"stock photo\", n=1, size=\"1024x1024\")\n",
    "        img_url = response['data'][0]['url']\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(f\"{save_dir}/dalle_{i}.jpg\", 'wb') as f:\n",
    "            f.write(img_data)\n",
    "\n",
    "generate_dalle_images(100, 'dalle_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(source, count):\n",
    "    downloaded = 0\n",
    "    while downloaded < count:\n",
    "        try:\n",
    "            # Example download process (adjust based on actual API)\n",
    "            downloaded += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    return downloaded\n",
    "\n",
    "sources = {'unsplash': 100, 'pexels': 100, 'flickr': 100}\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(download_batch, source, count) for source, count in sources.items()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        result = future.result()\n",
    "        print(f\"Completed batch with {result} downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(image_paths, labels, test_size=0.3, random_state=42)\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(temp_paths, temp_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPhotoDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockPhotoDetector, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)  # 2 classes: Real vs AI-generated\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_path = 'best_model.pth'\n",
    "\n",
    "    model = model.to(device)\n",
    "    wandb.init(project=\"stock-photo-detector\", name=\"training_run_v1\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = correct_val / total_val\n",
    "        print(f\"Train Loss: {running_loss / len(train_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "        scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
